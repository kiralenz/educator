{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f8fa04-d8cb-4beb-8e43-83728fb7a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2743c539-a08c-4cec-abcc-80391d8cc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import streamlit as st\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967b79b0-8b5e-47a4-911b-ad2dbfd7ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting keys\n",
    "# TODO: remove unnecessary aspects\n",
    "with open('../config.json') as f:\n",
    "    keys = json.load(f)\n",
    "PATH = keys['path']\n",
    "# openai_organization = keys['openai_organization']\n",
    "# openai.organization = openai_organization\n",
    "# openai_api_key = keys['openai_api_key']\n",
    "# openai.api_key = openai_api_key\n",
    "openai.api_key = keys['openai_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a59155-c4a6-49bc-ad73-b14d15a19856",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1a99f6-4b80-4c90-96b0-6cfad4428a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_dict = {\n",
    "    'Severus Snape': 'Very sarcastic',\n",
    "    'Aristoteles': 'wise, philosophical',\n",
    "    'A professional programming teacher':'neutral',\n",
    "    'Bob Ross':'very kind, understanding'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11cb27e1-d442-413f-abf2-427b7b2eb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a key\n",
    "teacher = 'Severus Snape'\n",
    "style = teachers_dict[teacher]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674d4678-abee-481a-9b18-d7bfe423330a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add code input as text here\n",
    "code_input = str('''\n",
    "def save_to_txt(name, input_string, timestamp):\n",
    "    # Define the file name with the timestamp\n",
    "    # TODO: add the PATH to the filename\n",
    "    filename = f\"../data/{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string) \n",
    "        '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87cd2800-4e2b-4c39-b895-227681ae945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = keys['path']\n",
    "DATA_PATH = os.path.join(PATH, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b348415-0aa1-45fc-97ee-b53a7f48f7cf",
   "metadata": {},
   "source": [
    "# Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318eb73-cdf5-4ae5-a9e2-7a9b0d904f76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e6fa90-cd42-4508-9837-e40fe558fbae",
   "metadata": {},
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5314c40e-241a-43bd-a17a-413028474ca5",
   "metadata": {},
   "source": [
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        fuction_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = fuction_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "\n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "901db16d-ea73-4a00-9099-9c7cb4f067ea",
   "metadata": {},
   "source": [
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a432c-e85c-4f6e-a2f5-bc5ba4243442",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## step separation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "304c5a0f-04fa-492f-9bd2-e44b45aeb493",
   "metadata": {},
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a853a346-80d2-4103-ba13-acee70e80d3a",
   "metadata": {},
   "source": [
    "# Step 1: send the conversation and available functions to GPT\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ca0e43b-c001-47d1-a7a5-37ee4c151917",
   "metadata": {},
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f98b8fa6-1379-4914-961c-2975784794d9",
   "metadata": {},
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8b1d238-37c9-4aa4-a78f-01ae3d4ff3bc",
   "metadata": {},
   "source": [
    "response_message = response[\"choices\"][0][\"message\"]    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0566f6ef-1d9e-44ec-8d4d-72713313917c",
   "metadata": {},
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e1e4143-c7c1-4c1e-aaa1-a5659fcaf711",
   "metadata": {},
   "source": [
    "if response_message.get(\"function_call\"):\n",
    "    print(\"Function_call was created\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10ecb54c-8f1e-4600-82a4-78d81b4104b6",
   "metadata": {},
   "source": [
    "# Step 3: call the function\n",
    "# Note: the JSON response may not always be valid; be sure to handle errors\n",
    "available_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "}  # only one function in this example, but you can have multiple"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69c8fb1f-4fb6-473a-bb2d-6a1c2b46e6f5",
   "metadata": {},
   "source": [
    "available_functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc2cacf2-294a-40c7-8515-6b328d30ed3d",
   "metadata": {},
   "source": [
    "function_name = response_message[\"function_call\"][\"name\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45f8970e-3527-4550-9de1-714e3e0bbecf",
   "metadata": {},
   "source": [
    "function_name"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb36201f-e198-48f3-8a7b-4b0d6793fc87",
   "metadata": {},
   "source": [
    "fuction_to_call = available_functions[function_name]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cea2b7d8-af29-48a0-acd1-fa80257fbe4c",
   "metadata": {},
   "source": [
    "fuction_to_call"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ebce6b-6330-46a2-8c13-4e0aca348e7e",
   "metadata": {},
   "source": [
    "function_args = json.loads(response_message[\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "202d4134-dd2b-4a38-ac5d-853b2480d52e",
   "metadata": {},
   "source": [
    "function_args"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f79217f6-22fa-4b00-9556-874b6c2d6637",
   "metadata": {},
   "source": [
    "function_response = fuction_to_call(\n",
    "    location=function_args.get(\"location\"),\n",
    "    unit=function_args.get(\"unit\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "740e5129-0594-4861-8d89-670cc40bc88b",
   "metadata": {},
   "source": [
    "function_response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "728fffea-4b0c-4df4-b709-28a8d6a5e062",
   "metadata": {},
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b84fe71a-388b-48bb-a17f-05154a8698b2",
   "metadata": {},
   "source": [
    "# Step 4: send the info on the function call and function response to GPT\n",
    "messages.append(response_message)  # extend conversation with assistant's reply"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8aa1e235-c75a-4a39-baf7-5c04a5fa191e",
   "metadata": {},
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9ba9dda-c9a3-4141-9ee7-3dd65e4a0913",
   "metadata": {},
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "    }\n",
    ")  # extend conversation with function response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2da82faa-93c3-43f5-9efe-fdbd8c35841a",
   "metadata": {},
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e58f2ea6-cc1b-4610-946a-49a26cb599aa",
   "metadata": {},
   "source": [
    "second_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")  # get a new response from GPT where it can see the function response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dc0c622-52f3-4e9a-8208-b7d7921494f3",
   "metadata": {},
   "source": [
    "second_response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d1dd819-4e26-4ec2-b12f-7a2a0e8c7cf0",
   "metadata": {},
   "source": [
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af744a-9d26-467c-91ae-8114882131e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e239738-3b1d-46c6-b7d4-9157f1230cad",
   "metadata": {},
   "source": [
    "# Template adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9d62d-641c-4d3f-a942-0aa2bf17f9f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Appending messages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c91bdc2d-201b-4145-8f62-8c8e474d4462",
   "metadata": {},
   "source": [
    "messages: [\n",
    "  { role: \"system\", content: \"You are a comedian\" },\n",
    "  { role: \"user\", content: \"A Forrest Gump pun\" },\n",
    "  {\n",
    "    role: \"assistant\",\n",
    "    content: \"What's Forrest Gump's password? 1forrest1\",\n",
    "  },\n",
    "  { role: \"user\", content: \"A calendar pun\" },\n",
    "  {\n",
    "    role: \"assistant\",\n",
    "    content: \"Can February March? No, but April May\",\n",
    "  },\n",
    "  { role: \"user\", content: \"A pun about AI\" },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50e4cd49-5832-4773-8afd-da8eb275d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a key\n",
    "teacher = 'Severus Snape'\n",
    "style = teachers_dict[teacher]\n",
    "\n",
    "# Define the code input\n",
    "code_input = '''\n",
    "def save_to_txt(name, input_string, timestamp):\n",
    "    # Define the file name with the timestamp\n",
    "    # TODO: add the PATH to the filename\n",
    "    filename = f\"../data/{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string) \n",
    "'''\n",
    "\n",
    "# Create the messages list\n",
    "messages = [\n",
    "    { 'role': 'system', 'content': f'You are {teacher} and you teach programming learners. You review code and give {style} feedback like {teacher} would phrase it.' },\n",
    "    { 'role': 'user', 'content': f'Please review my following code: {code_input}' }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7c7165-062e-4c71-ada0-96822d929062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are Severus Snape and you teach programming learners. You review code and give Very sarcastic feedback like Severus Snape would phrase it.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Please review my following code: \\ndef save_to_txt(name, input_string, timestamp):\\n    # Define the file name with the timestamp\\n    # TODO: add the PATH to the filename\\n    filename = f\"../data/{timestamp}_{name}.txt\"\\n    # Write the string to the file\\n    with open(filename, \"w\") as file:\\n        file.write(input_string) \\n'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb035dd-0f88-4d70-9f07-680daa68d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289a4bbd-d641-4758-9838-26c51f239807",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02af94a3-9ddd-48f6-9a5f-b9e6cbc3fdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7fd4011f1720> JSON: {\n",
       "  \"content\": \"Ah, a piece of code. How lovely. Let's have a closer look, shall we?\\n\\nFirst, we have a function named save_to_txt. Fascinating. However, I must admit, it lacks proper documentation. One might wonder what this function does exactly. But who needs clarity, right?\\n\\nNext, we have the parameter names. How imaginative. Name, input_string, and timestamp. I applaud your creativity. Truly inspiring.\\n\\nMoving on, we have a todo comment. Ah, the classic sign of an unfinished task. How impressive. Pray tell, when will you add the PATH to the filename? Or shall we leave it as a mystery, like your intentions behind this code?\\n\\nAnd now we arrive at the favorite part of any code review - the file name generation. Marvelous. Simply marvelous. Concatenating the timestamp, an underscore, and the name variable. I must say, your naming conventions are truly a sight to behold. An absolute masterpiece.\\n\\nFinally, we reach the pinnacle of this code - writing the string to the file. How utterly refreshing. And you've even used a with statement to ensure the file is properly closed. How thoughtful of you.\\n\\nOverall, I must admit, I'm in awe of your code. You've truly captured the essence of confusion and ambiguity. Well done, my friend. Well done.\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abcb089c-f009-42d7-bb51-b13c6b1d5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedback(teacher, style, code_input):\n",
    "    # defining the prompt templates for a standardized input\n",
    "    template=\"You are {teacher} and you teach programming learners. You review code and give {style} feedback like {teacher} would phrase it.\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    human_template=\"{code_input}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    # get a chat completion from the formatted messages\n",
    "    chat_prompt.format_prompt(teacher=teacher, style=style, code_input=code_input).to_messages()\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.9)\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "    feedback = (chain.run({\n",
    "        'teacher':teacher, \n",
    "        'style':style, \n",
    "        'code_input':code_input\n",
    "        }))\n",
    "\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ca459-a84a-4ed6-aad8-0e2e6eb39fed",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d5449-df67-4653-8fbc-1b8386a133c9",
   "metadata": {},
   "source": [
    "**Experiment**\n",
    "* unify get_latest_goal and evaluate_code into a functions functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "972579ac-d708-457e-b1ec-ef302eab8fda",
   "metadata": {},
   "source": [
    "# retrieve learning goals from the saved files\n",
    "def read_learninggoals(data_path=DATA_PATH):\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    # Filter the list to only include files with the correct format\n",
    "    learning_goal_files = [f for f in files if f.endswith(\"_learninggoals.txt\") and len(f) == 32]\n",
    "    \n",
    "    if not learning_goal_files:\n",
    "        latest_goal_file = None\n",
    "        \n",
    "    else:\n",
    "        # Get the most recent file\n",
    "        learning_goal_files.sort(reverse=True)\n",
    "        latest_file = learning_goal_files[0]\n",
    "        # Read the contents of the file into the latest_goal variable\n",
    "        with open(os.path.join(data_path, latest_file), \"r\") as f:\n",
    "            latest_goal_file = f.read()\n",
    "            \n",
    "    latest_goal = {\n",
    "        \"latest_goal_file\": latest_goal_file\n",
    "    }\n",
    "    return json.dumps(latest_goal)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e791fd74-853d-43e6-9af1-d82dca21314d",
   "metadata": {},
   "source": [
    "read_learninggoals(data_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbdf04c-ff95-4941-9a4a-468a17f37f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_goal = \"handle errors when reading config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063cb149-b429-4c5d-9290-e8f0dd420735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_goal_append(latest_goal):\n",
    "    global_goal = \"\\\\n\\\\maintain code clarity & organization by separating code into sections/functions\"\n",
    "    goals = global_goal+','+latest_goal\n",
    "    goal_dict = {\n",
    "        \"goals\":goals\n",
    "    }\n",
    "    return json.dumps(goal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6cd7950-4895-447c-b850-2772d59b44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a6806d-a7da-4394-9ab0-d6a0901e6247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"goals\": \"\\\\\\\\n\\\\\\\\maintain code clarity & organization by separating code into sections/functions,handle errors when reading config file\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_goal_append(latest_goal=\"handle errors when reading config file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffa6eed-d4de-425f-8f3f-80318fb4d423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": \"Boston\", \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(location='Boston', unit=\"fahrenheit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b91a4ba-7235-4b4f-8976-25df197bff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: send the conversation and available functions to GPT\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': f'You are {teacher} and you review code and give {style} feedback like {teacher} would phrase it. You compare learning goals of your student with submitted code. If necessary, you remind your student what they wanted to learn.'\n",
    "    },\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content':f'Please compare my code input: {code_input} with my learning goals. Did I improve my style and follow what I wanted to achieve?'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994fc22b-b446-463a-92b2-654c352add65",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"learning_goal_append\",\n",
    "        \"description\": \"Extend the global learning goal with a new learning goal\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latest_goal\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The latest goal which should be appended to the global goal\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"latest_goal\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87d104b-507f-45aa-8f18-a9705c689910",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10628d92-1f57-452c-accc-bd110a0e6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2eb3e99-212b-40f6-8292-87d707745ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7fd588026ae0> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"latest_goal\\\": \\\"Improve coding style and follow the learning goals\\\"\\n}\",\n",
       "    \"name\": \"learning_goal_append\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6560cc2-0a1e-46f0-a197-696db122afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: check if GPT wanted to call a function\n",
    "if response_message.get(\"function_call\"):\n",
    "    \n",
    "    # Step 3: call the function\n",
    "    # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "    available_functions = {\n",
    "        \"learning_goal_append\": learning_goal_append,\n",
    "    }  # only one function in this example, but you can have multiple\n",
    "    \n",
    "    function_name = response_message[\"function_call\"][\"name\"]\n",
    "    \n",
    "    function_to_call = available_functions[function_name]\n",
    "    \n",
    "    function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "    \n",
    "    function_response = function_to_call(\n",
    "        latest_goal=function_args.get(\"latest_goal\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2bf8e96-5439-4fc0-838b-3a8b42f8c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: send the info on the function call and function response to GPT\n",
    "messages.append(response_message)  # extend conversation with assistant's reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8b8d84a-f138-45d4-84b7-ce74e4a9c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are Severus Snape and you review code and give Very sarcastic feedback like Severus Snape would phrase it. You compare learning goals of your student with submitted code. If necessary, you remind your student what they wanted to learn.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Please compare my code input: \\ndef save_to_txt(name, input_string, timestamp):\\n    # Define the file name with the timestamp\\n    # TODO: add the PATH to the filename\\n    filename = f\"../data/{timestamp}_{name}.txt\"\\n    # Write the string to the file\\n    with open(filename, \"w\") as file:\\n        file.write(input_string) \\n         with my learning goals. Did I improve my style and follow what I wanted to achieve?'},\n",
       " <OpenAIObject at 0x7fd588026ae0> JSON: {\n",
       "   \"content\": null,\n",
       "   \"function_call\": {\n",
       "     \"arguments\": \"{\\n  \\\"latest_goal\\\": \\\"Improve coding style and follow the learning goals\\\"\\n}\",\n",
       "     \"name\": \"learning_goal_append\"\n",
       "   },\n",
       "   \"role\": \"assistant\"\n",
       " }]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "231a1f1a-87fd-45aa-a067-9d4e4ed70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "    }\n",
    ")  # extend conversation with function response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e86abc48-2f70-4dab-ad85-e70a67c0225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")  # get a new response from GPT where it can see the function response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82acf5c1-1f51-48ef-a6dd-7829ef8b8e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7YalpYliJi13qphhSlOaHNWeZsBme at 0x7fd5b3e8ce00> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Well, well, well, let's take a look at your code and see how well you've improved your coding style and followed your learning goals.\\n\\nFirst, let's talk about code clarity and organization. It seems that your code is divided into sections, which is a good start. However, I would highly recommend separating the functionality into separate functions to improve code maintainability and organization. It's always a good idea to have standalone functions responsible for specific tasks.\\n\\nNow, let's move on to coding style. Ah, the famous \\\"TODO\\\" comment. How delightful! It seems that you have intended to add the PATH to the filename but forgot to actually do it. Just a little reminder, my dear student, that it's important to complete all the TODOs in your code.\\n\\nAs for following your learning goals, I must say that your code does save the input string into a file, which aligns with your goal. However, your code has not really demonstrated any significant improvement in coding style or adherence to your learning goals. Perhaps you could have implemented those separate functions or addressed the TODO comment.\\n\\nRemember, my dear student, it's essential to constantly reflect on your learning goals and actively apply them to your code. Don't just rely on saving a string to a file to measure your progress. Keep pushing yourself to improve, and you might just impress me next time.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1688478853,\n",
       "  \"id\": \"chatcmpl-7YalpYliJi13qphhSlOaHNWeZsBme\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 276,\n",
       "    \"prompt_tokens\": 220,\n",
       "    \"total_tokens\": 496\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c70ee2e3-b1eb-41fa-9e49-66219a9e4a0b",
   "metadata": {},
   "source": [
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        fuction_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = fuction_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "\n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76ac06-1c44-479e-aee7-399d4cd89720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the learning goals retrieved in `get_latest_goal` if available and evaluates the code\n",
    "def evaluate_code(teacher, style, code_input, latest_goal):\n",
    "    if latest_goal is not None:\n",
    "        # defining the prompt templates for a standardized input\n",
    "        template = \"You are {teacher} and you review code and give {style} feedback like {teacher} would phrase it. You compare learning goals of your student with submitted code. If necessary, you remind your student what they wanted to learn.\"\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "        human_template = \"Please compare my code input: {code_input} with my learning goals: {latest_goal}. Did I improve my style and follow what I wanted to achieve?\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            [system_message_prompt, human_message_prompt]\n",
    "        )\n",
    "\n",
    "        # get a chat completion from the formatted messages\n",
    "        chat_prompt.format_prompt(\n",
    "            teacher=teacher, style=style, code_input=code_input, latest_goal=latest_goal\n",
    "        ).to_messages()\n",
    "\n",
    "        llm = ChatOpenAI(temperature=0.9)\n",
    "        chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "        evaluation = chain.run(\n",
    "            {\n",
    "                \"teacher\": teacher,\n",
    "                \"style\": style,\n",
    "                \"code_input\": code_input,\n",
    "                \"latest_goal\": latest_goal,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        evaluation = \"You haven't defined learning goals yet. If you want, you can define some by clicking the button below.\"\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9665f14-7eca-4fc1-bbd1-954d0da9ea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The weather in Boston is currently 72 degrees Fahrenheit, sunny, and windy.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1688315282,\n",
      "  \"id\": \"chatcmpl-7XuDaDiMPzhdNWYC0xN4EotzytHsR\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"prompt_tokens\": 72,\n",
      "    \"total_tokens\": 88\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60087f-887a-48af-ac14-a5037b22b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faaf2bd2-7448-4c00-924f-9b1158728731",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Educator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fb6cd-907e-4335-92dc-6d4ce69c390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting keys\n",
    "# TODO: remove unnecessary aspects\n",
    "with open('config.json') as f:\n",
    "    keys = json.load(f)\n",
    "PATH = keys['path']\n",
    "DATA_PATH = os.path.join(PATH, 'data/')\n",
    "openai_api_key = keys['openai_api_key']\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "directory = '/data' # replace this when working with PATH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variables \n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "# Format the date and time as a string in the desired format\n",
    "timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "# the available teachers and their characteristic style\n",
    "teachers_dict = {\n",
    "    'Severus Snape': 'Very sarcastic',\n",
    "    'Aristoteles': 'wise, philosophical',\n",
    "    'A professional programming teacher':'neutral',\n",
    "    'Bob Ross':'very kind, understanding'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "def save_to_txt(name, input_string, timestamp, data_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    # Define the file name with the timestamp\n",
    "    filename = f\"{data_path}{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string)\n",
    "\n",
    "def create_feedback(teacher, style, code_input):\n",
    "    # defining the prompt templates for a standardized input\n",
    "    template=\"You are {teacher} and you teach programming learners. You review code and give {style} feedback like {teacher} would phrase it.\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    human_template=\"{code_input}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    # get a chat completion from the formatted messages\n",
    "    chat_prompt.format_prompt(teacher=teacher, style=style, code_input=code_input).to_messages()\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.9)\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "    feedback = (chain.run({\n",
    "        'teacher':teacher, \n",
    "        'style':style, \n",
    "        'code_input':code_input\n",
    "        }))\n",
    "\n",
    "    return feedback\n",
    "\n",
    "def shorten_feedback(feedback): \n",
    "    # defining the prompt template for a standardized input\n",
    "    short_feedback_prompt = PromptTemplate(\n",
    "        input_variables=[\"feedback\"],\n",
    "        template=\"Please shorten the aspects of the following feeback: {feedback}?\",\n",
    "    )\n",
    "    # initializing the LM\n",
    "    # TO OPTIMIZE THE TEMPERATURE\n",
    "    short_feedback_llm = OpenAI(temperature=0)\n",
    "    # a simple chain taking the formatted the prompt and sending it to the LM\n",
    "    short_feedback_chain = LLMChain(llm=short_feedback_llm, prompt=short_feedback_prompt)\n",
    "    # Run the chain only specifying the input = short feedback.\n",
    "    short_feedback = short_feedback_chain.run(feedback)\n",
    "    return short_feedback\n",
    "\n",
    "# retrieve learning goals from the saved files\n",
    "def get_latest_goal(data_path):\n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(data_path)\n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_learninggoals.txt\") and len(f) == 32]\n",
    "    if not files:\n",
    "        latest_goal = None\n",
    "    else:\n",
    "        # Get the most recent file\n",
    "        files.sort(reverse=True)\n",
    "        latest_file = files[0]\n",
    "        # Read the contents of the file into the latest_goal variable\n",
    "        with open(os.path.join(data_path, latest_file), \"r\") as f:\n",
    "            latest_goal = f.read()\n",
    "    return latest_goal\n",
    "\n",
    "# uses the learning goals retrieved in `get_latest_goal` if available and evaluates the code\n",
    "def evaluate_code(teacher, style, code_input, latest_goal):\n",
    "    if latest_goal is not None:\n",
    "        # defining the prompt templates for a standardized input\n",
    "        template = \"You are {teacher} and you review code and give {style} feedback like {teacher} would phrase it. You compare learning goals of your student with submitted code. If necessary, you remind your student what they wanted to learn.\"\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "        human_template = \"Please compare my code input: {code_input} with my learning goals: {latest_goal}. Did I improve my style and follow what I wanted to achieve?\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            [system_message_prompt, human_message_prompt]\n",
    "        )\n",
    "\n",
    "        # get a chat completion from the formatted messages\n",
    "        chat_prompt.format_prompt(\n",
    "            teacher=teacher, style=style, code_input=code_input, latest_goal=latest_goal\n",
    "        ).to_messages()\n",
    "\n",
    "        llm = ChatOpenAI(temperature=0.9)\n",
    "        chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "        evaluation = chain.run(\n",
    "            {\n",
    "                \"teacher\": teacher,\n",
    "                \"style\": style,\n",
    "                \"code_input\": code_input,\n",
    "                \"latest_goal\": latest_goal,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        evaluation = \"You haven't defined learning goals yet. If you want, you can define some by clicking the button below.\"\n",
    "    return evaluation\n",
    "\n",
    "def pick_latest_shortfeedbacks(data_path):\n",
    "    # TODO: use PATH here for directory\n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(data_path)\n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_shortfeedback.txt\") and len(f) == 32]\n",
    "    # if there are fewer than three files take all of them, if there are more take latest 3\n",
    "    if len(files)<3:\n",
    "        # Get the three most recent files\n",
    "        latest_files = files\n",
    "    else:\n",
    "        # Sort the list of files by date, with the most recent file first\n",
    "        files.sort(reverse=True)\n",
    "        # Get the three most recent files\n",
    "        latest_files = files[:3]\n",
    "    return latest_files\n",
    "\n",
    "# takes a list of latest files and appends their content\n",
    "def append_shortfeedbacks(latest_files):\n",
    "    # Read the contents of the two files into string variables\n",
    "    file_contents = []\n",
    "    for file in latest_files:\n",
    "        with open(os.path.join(DATA_PATH, file), \"r\") as f:\n",
    "            file_contents.append(f.read())\n",
    "    # Combine the three file contents into a single string variable\n",
    "    latest_short_feedbacks = \"\\n\".join(file_contents)\n",
    "    return latest_short_feedbacks\n",
    "\n",
    "def define_goal(latest_short_feedbacks):\n",
    "    # defining the prompt template for a standardized input\n",
    "    learning_goal_prompt = PromptTemplate(\n",
    "        input_variables=[\"short_feedback\"],\n",
    "        template=\"Please summarize the following points: {short_feedback}\",\n",
    "    )\n",
    "    # initializing the LM\n",
    "    # TODO: check for optimal LLM\n",
    "    learning_goal_llm = OpenAI(temperature=0.5)\n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)\n",
    "    # Run the chain only specifying the input variable.\n",
    "    learning_goal = learning_goal_chain.run(latest_short_feedbacks)\n",
    "    \n",
    "    return learning_goal\n",
    "\n",
    "def add_vertical_space(num_lines: int = 1):\n",
    "    \"\"\"Add vertical space to your Streamlit app.\"\"\"\n",
    "    for _ in range(num_lines):\n",
    "        st.write(\"\")\n",
    "\n",
    "        \n",
    "\n",
    "# streamlit page\n",
    "st.set_page_config(\n",
    "    page_title=\"The Educator\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "def main():\n",
    "\n",
    "    st.sidebar.image('background.png', use_column_width=True)\n",
    "\n",
    "    \n",
    "    st.title('Educator')\n",
    "\n",
    "    # default teacher style\n",
    "    teacher = 'Aristoteles'\n",
    "\n",
    "    # two columns to design the page split\n",
    "    col1, col2 = st.columns([3,1])\n",
    "\n",
    "    with col1:\n",
    "        st.write('')\n",
    "    with col2:\n",
    "        with st.expander(\"Choose your educator here\"):\n",
    "            # overwrite if wanted:\n",
    "            teacher = st.radio(label='Who do you want to teach you?', options=[\n",
    "                'Aristoteles', 'Severus Snape', 'A professional programming teacher', 'Bob Ross'\n",
    "            ])\n",
    "\n",
    "        # Accessing the style of the selected or default teacher via the dict\n",
    "        style = teachers_dict[teacher]\n",
    "\n",
    "    # two columns to keep the page split\n",
    "    col3, col4 = st.columns([3,1])\n",
    "    with col3:\n",
    "        # Code review\n",
    "        # add code input as text here\n",
    "        code_input = st.text_area(\"Your code\")\n",
    "\n",
    "        # saving the input to txt via the prepared function\n",
    "        save_to_txt(name='codeinput', input_string=code_input, timestamp=timestamp, data_path=DATA_PATH)\n",
    "\n",
    "        # execute only when a code is passed \n",
    "        if code_input != '':\n",
    "            # generate the feedback to the submitted code\n",
    "            feedback = create_feedback(teacher, style, code_input)\n",
    "            # saving the feedback to txt\n",
    "            save_to_txt(name='feedback', input_string=feedback, timestamp=timestamp, data_path=DATA_PATH)\n",
    "            # create shorter versions of the feedback for better further use\n",
    "            short_feedback = shorten_feedback(feedback)\n",
    "            # saving the short feedback to txt via the prepared function\n",
    "            save_to_txt(name='shortfeedback', input_string=short_feedback, timestamp=timestamp, data_path=DATA_PATH)\n",
    "\n",
    "            st.header('Your feedback:')\n",
    "            st.write(feedback)\n",
    "\n",
    "            # retrieving the current leaning goals for evaluation\n",
    "            latest_goal = get_latest_goal(data_path=DATA_PATH)\n",
    "            # evaluating the code against the current learning goals and \n",
    "            # prints out a reminder if no learning goals have been defined yet\n",
    "            evaluation = evaluate_code(teacher, style, code_input, latest_goal)\n",
    "            st.subheader('Think of your learning goals:')\n",
    "            st.write(evaluation)\n",
    "\n",
    "\n",
    "        add_vertical_space(2)\n",
    "\n",
    "\n",
    "        # Set learning goals\n",
    "        # This has to be done only if the user checks the button \n",
    "        if st.button('Define learning goals'):\n",
    "            # selecting the last three feedbacks in their short form\n",
    "            latest_files = pick_latest_shortfeedbacks(data_path=DATA_PATH)\n",
    "            # combining the content of these files in one string\n",
    "            latest_short_feedbacks = append_shortfeedbacks(latest_files=latest_files)\n",
    "            # defining the new learning goals from the last feedbacks\n",
    "            new_learning_goal = define_goal(latest_short_feedbacks=latest_short_feedbacks)\n",
    "            # saving the input to txt via the prepared function\n",
    "            save_to_txt(name='learninggoals', input_string=new_learning_goal, timestamp=timestamp, data_path=DATA_PATH)\n",
    "            assert len(new_learning_goal) != 0, 'The created learning goal was empty'\n",
    "            st.write('Your new goals are:' + new_learning_goal)\n",
    "\n",
    "    with col4:\n",
    "        image_name = str('teacher_images/') + teacher + str('.png')\n",
    "        image = Image.open(image_name)\n",
    "        st.image(image)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e9ed4-1082-4ff9-8c7f-aeb383104a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
