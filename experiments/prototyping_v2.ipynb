{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f8fa04-d8cb-4beb-8e43-83728fb7a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2743c539-a08c-4cec-abcc-80391d8cc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import base64\n",
    "import os\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967b79b0-8b5e-47a4-911b-ad2dbfd7ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting keys\n",
    "# TODO: remove unnecessary aspects\n",
    "with open('../config.json') as f:\n",
    "    keys = json.load(f)\n",
    "PATH = keys['path']\n",
    "openai_organization = keys['openai_organization']\n",
    "openai.organization = openai_organization\n",
    "openai_api_key = keys['openai_api_key']\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f515a5-a463-4075-a574-d55e4357bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cdcb3a-957d-4115-9a92-b5d63f73adae",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c89ceb2-6081-4861-a93c-72a3e9e37a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24478b90-b9af-429f-aabd-c179350b50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time as a string in the desired format\n",
    "timestamp = now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e309e23-205f-4c79-8683-b1dbc7d72692",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12a0850-0c0d-4cf3-81db-ffd7ec84b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(name, input_string):\n",
    "    # Define the file name with the timestamp\n",
    "    # TODO: add the PATH to the filename\n",
    "    filename = f\"../data/{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f54b07-ef52-4970-aaa5-3b288ab85078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedback(code_input):\n",
    "    # defining the prompt template for a standardized input\n",
    "    # TO EXPLORE: refine prompt\n",
    "    feedback_prompt = PromptTemplate(\n",
    "        input_variables=[\"code\"],\n",
    "        template=\"Please review the following code and give five recommendations with detailed explanations how to improve the programming: {code}?\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TO EXPLORE: adjust temperature\n",
    "    # TO EXPLORE: test other LMs\n",
    "    feedback_llm = OpenAI(temperature=0.3)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    feedback_chain = LLMChain(llm=feedback_llm, prompt=feedback_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    feedback = feedback_chain.run(code_input)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='feedback', input_string=feedback)\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15177b15-a44a-4c65-95ea-24708bee1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_feedback(feedback):\n",
    "    # defining the prompt template for a standardized input\n",
    "    short_feedback_prompt = PromptTemplate(\n",
    "        input_variables=[\"feedback\"],\n",
    "        template=\"Please shorten the aspects of the following feeback: {feedback}?\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TO OPTIMIZE\n",
    "    short_feedback_llm = OpenAI(temperature=0)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    short_feedback_chain = LLMChain(llm=short_feedback_llm, prompt=short_feedback_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    short_feedback = short_feedback_chain.run(feedback)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='shortfeedback', input_string=short_feedback)\n",
    "    \n",
    "    return short_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3b0edb-a8f7-4ff5-a60d-1bd9aac5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: separate into a reading and a merging function and reuse reading function\n",
    "def pick_shortfeedbacks(directory, n):\n",
    "    # TODO: use PATH here for directory\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_shortfeedback.txt\") and len(f) == 32]\n",
    "    \n",
    "    # Sort the list of files by date, with the most recent file first\n",
    "    files.sort(reverse=True)\n",
    "    \n",
    "    # Get the three most recent files\n",
    "    latest_files = files[:n]\n",
    "    \n",
    "    # Read the contents of the two files into string variables\n",
    "    file_contents = []\n",
    "    for file in latest_files:\n",
    "        with open(os.path.join(directory, file), \"r\") as f:\n",
    "            file_contents.append(f.read())\n",
    "            \n",
    "    # Combine the two file contents into a single string variable\n",
    "    latest_short_feedbacks = \"\\n\".join(file_contents)\n",
    "    \n",
    "    return latest_short_feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d46fb2-d747-4309-b694-a367b321429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: improve the template without making it break\n",
    "def define_goal(latest_short_feedbacks):\n",
    "    \n",
    "    # defining the prompt template for a standardized input\n",
    "    learning_goal_prompt = PromptTemplate(\n",
    "        input_variables=[\"short_feedback\"],\n",
    "        template=\"Summarize the following points: {short_feedback}\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TODO: check for optimal LLM\n",
    "    learning_goal_llm = OpenAI(temperature=0.5)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    learning_goals = learning_goal_chain.run(latest_short_feedbacks)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='learninggoals', input_string=learning_goals)\n",
    "    \n",
    "    return learning_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29074fe3-3474-4324-a2bc-77cd44ecff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code(directory, code_input):\n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_learninggoals.txt\") and len(f) == 32]\n",
    "\n",
    "    # Sort the list of files by date, with the most recent file first\n",
    "    files.sort(reverse=True)\n",
    "\n",
    "    # Get the most recent file\n",
    "    latest_file = files[:1]\n",
    "\n",
    "    with open(os.path.join(directory, latest_file[0]), \"r\") as f:\n",
    "        learning_goals = f.read()\n",
    "    \n",
    "    # defining the promp template to get both the latest code input and learning_goals\n",
    "    evaluation_prompt = PromptTemplate(\n",
    "        input_variables=[\"code_input\", \"learning_goals\"],\n",
    "        template=\"Please compare this code: {code_input} with these learning goals: {learning_goals}. If the programmer considered the learning goals when writing the provided code, say something motivating. If the programmer didn't consider the learning goals, gently remind the person of their learning goals.\",\n",
    "    )\n",
    "    \n",
    "    # application of the template\n",
    "    evaluation_prompt = evaluation_prompt.format(code_input=code_input, learning_goals=learning_goals)\n",
    "    \n",
    "    # llm definition\n",
    "    evaluation_llm = OpenAI(temperature=0.3)\n",
    "    \n",
    "    evaluation = evaluation_llm(evaluation_prompt)\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2abdf3-54d5-4ee8-ac56-fec41e861e3d",
   "metadata": {},
   "source": [
    "# Code review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417fd72-8724-45e6-ad33-d74e1b6fc200",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7362f9-f9f2-43f3-b704-0e5368ca8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature to be added: read from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "674d4678-abee-481a-9b18-d7bfe423330a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add code input as text here\n",
    "code_input = str('''\n",
    "    # Libraries\n",
    "    import streamlit as st\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import base64\n",
    "\n",
    "    # getting variables from config.json\n",
    "    with open('config/config.json') as f:\n",
    "        keys = json.load(f)\n",
    "    PATH = keys['path']\n",
    "\n",
    "\n",
    "    # Functions\n",
    "    # better read functions from utils, but not yet working\n",
    "    def add_bg():\n",
    "        st.markdown(\n",
    "        f\"\"\"\n",
    "        <style>\n",
    "        .stApp {{\n",
    "            background-image: url(https://gist.githubusercontent.com/kiralenz/8fa216a5ab87e92944129da83d84dd5b/raw/806c89b90ee9c6eaf75f833eb9482c9cbca7dec1/bread_loaf.svg);\n",
    "            background-size: cover\n",
    "        }}\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "        )\n",
    "\n",
    "    def add_logo(height):\n",
    "        st.markdown(\n",
    "            f\"\"\"\n",
    "            <style>\n",
    "                [data-testid=\"stSidebarNav\"] {{\n",
    "                    background-image: url(https://gist.githubusercontent.com/kiralenz/16203a45856cfb596741f24f85e82fbe/raw/c9d93e3336730e77132d40df4eb8d758471bcfd8/keeprising_logo.svg);\n",
    "                    background-repeat: no-repeat;\n",
    "                    padding-top: {height - 40}px;\n",
    "                    background-position: 20px 20px;\n",
    "                }}\n",
    "            </style>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True,\n",
    "        )\n",
    "\n",
    "    # merging historical activities (df_hist) with latest activity data (df_new) \n",
    "    # on the target or shared date column (date_column)\n",
    "    def add_latest_activity(df_hist, df_new, date_column):\n",
    "        # Fixing dtypes\n",
    "        df_hist[date_column] = df_hist[date_column].astype(str)\n",
    "        df_new[date_column] = df_new[date_column].astype(str)\n",
    "\n",
    "        # Df merging of historical feedings and latest feeding\n",
    "        df = pd.concat([df_hist, df_new], ignore_index=True)\n",
    "        # Fixing dtypes and formatting\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[date_column] = df[date_column].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        return df\n",
    "\n",
    "    # adding a column with the microbial composition based on the feeding temperature\n",
    "    def bacteria_column(df, bac_compos):\n",
    "        df['bacteria_composition'] = np.where(\n",
    "            df[\"temperature\"] <= 20,\n",
    "            bac_compos.loc[\n",
    "                bac_compos[\"temperature\"] == 20, \"dominant_microbes\"\n",
    "            ],\n",
    "            np.where(\n",
    "                ((df[\"temperature\"] > 20) & (df[\"temperature\"] <= 25)),\n",
    "                bac_compos.loc[\n",
    "                    bac_compos[\"temperature\"] == 25, \"dominant_microbes\"\n",
    "                ],\n",
    "                np.where(\n",
    "                    ((df[\"temperature\"] > 25) & (df[\"temperature\"] <= 30)),\n",
    "                    bac_compos.loc[\n",
    "                        bac_compos[\"temperature\"] == 30, \"dominant_microbes\"\n",
    "                    ],\n",
    "                    bac_compos.loc[\n",
    "                        bac_compos[\"temperature\"] == 35, \"dominant_microbes\"\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # adding two columns for growth rates to a dataframe, one is time normalized\n",
    "    def growth_rate_cols(df):\n",
    "        df['growth_rate'] = (\n",
    "            df['end_height'] / df['initial_height']\n",
    "        )\n",
    "\n",
    "        df['growth_rate_per_hour'] = (\n",
    "            df['end_height'] \n",
    "            / df['initial_height'] \n",
    "            / df['feeding_time']\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # Loading data\n",
    "    feedings = pd.read_parquet(PATH + 'feedings.parquet')\n",
    "    bacteria_composition = pd.read_parquet(PATH + 'bacteria_composition.parquet')\n",
    "\n",
    "\n",
    "    # streamlit page\n",
    "    st.set_page_config(page_title=\"Keeprising\")\n",
    "    add_bg()  \n",
    "    add_logo(height=160)\n",
    "    st.title('How was your last feeding?') \n",
    "\n",
    "\n",
    "    # Adding new feeding data\n",
    "    # user input for feeding\n",
    "    date_today = st.date_input('Feeding date')\n",
    "    temperature_today = st.number_input('Temperature')\n",
    "    feeding_time_today = st.number_input('Feeding duration')\n",
    "    initial_height_today = st.number_input('Intial height')\n",
    "    end_height_today = st.number_input('End height')\n",
    "    bubble_size_today = st.number_input('Bubble size')\n",
    "\n",
    "    # error handling for invalid input\n",
    "    if temperature_today < 0 or feeding_time_today < 0 or initial_height_today < 0 or end_height_today < 0 or end_height_today < initial_height_today:\n",
    "        st.error('Invalid input! Please enter valid values for all feeding data. IF these had been your actual values consider immediately repeating the feeding to save your starter!')\n",
    "    else:\n",
    "        # storing latest information in a df\n",
    "        latest_feeding = pd.DataFrame(data={\n",
    "            'feeding_date':date_today, \n",
    "            'temperature':temperature_today,\n",
    "            'feeding_time':feeding_time_today,\n",
    "            'initial_height':initial_height_today,\n",
    "            'end_height':end_height_today,\n",
    "            'bubble_size':bubble_size_today\n",
    "        }, index=[0])\n",
    "\n",
    "        # merging new feeding to history of feedings\n",
    "        feedings = add_latest_activity(df_hist=feedings, df_new=latest_feeding, date_column='feeding_date')\n",
    "\n",
    "        # saving df to local file\n",
    "        feedings.to_parquet(PATH + 'feedings.parquet')\n",
    "\n",
    "        # application display of latest feedings\n",
    "        st.dataframe(feedings.tail())\n",
    "        st.write(\"Nice job! Well done!\")\n",
    "\n",
    "\n",
    "        # Data processing\n",
    "        feedings_processed = feedings.copy()\n",
    "        # Bacteria composition depending on temperature\n",
    "        feedings_processed = bacteria_column(df=feedings_processed, bac_compos=bacteria_composition)\n",
    "        # Growth rate composition\n",
    "        feedings_processed = growth_rate_cols(df=feedings_processed)\n",
    "\n",
    "\n",
    "        # Storing data\n",
    "        feedings_processed.to_parquet(PATH + 'feedings_processed.parquet')\" \n",
    "        '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f0ca9b8-42d6-4b83-bbb4-6220a4a8dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='codeinput', input_string=code_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b290768-fe8b-444d-a153-f8254eee7eac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ff75d8-d970-4625-bdd6-b29f81543387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.8 ms, sys: 5.06 ms, total: 36.8 ms\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feedback = create_feedback(code_input=code_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1115ac1-9b04-4107-9d4c-e6f4e032f606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. To improve readability, the code should be broken down into smaller, more manageable functions. For example, the “add_latest_activity” function can be broken down into two separate functions, one to fix the data types and one to merge the data frames. \\n\\n2. To improve readability, the code should be organized into sections with clear headings. For example, the code can be organized into sections for “Libraries”, “Functions”, “Loading Data”, “Streamlit Page”, “Adding New Feeding Data”, and “Data Processing”. \\n\\n3. To improve readability, the code should use more descriptive variable names. For example, instead of using “df” as a variable name, a more descriptive name such as “data_frame” should be used. \\n\\n4. To improve readability, the code should use more consistent indentation and spacing. For example, the lines of code within the “add_latest_activity” function should all be indented by the same amount. \\n\\n5. To improve readability, the code should use more'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c035e92-0dc4-4592-899d-95646c149214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the program if the created output is empty\n",
    "assert len(feedback) != 0, 'The created feedback was empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6affaef-0216-47b9-b47b-af85334e5d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76330750-b652-4bb5-9b0e-01eebadaaf14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning goals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5619e8-5410-48cc-a2f1-9f6276e3134b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shorten review for learning target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e681a5b9-afb2-42ee-adfe-83aa44784d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 ms, sys: 3.53 ms, total: 14.8 ms\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "short_feedback = shorten_feedback(feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889053b-a8f7-4206-b625-35df623fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the program if the created output is empty\n",
    "assert len(short_feedback) != 0, 'The created shortfeedback was empty'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536255f-8a9e-4a33-88c3-9d0180ee0c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set learning goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c2339-7420-4d67-a090-ba53ec4b1d67",
   "metadata": {},
   "source": [
    "### prep: get latest shortfeedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cbd79ef-9256-4ba2-a5bc-d298ea5e8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be done once and then only if the user checks a button like \"generate learning targets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6fe1f39-3180-410e-b551-88edadd7df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "button = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7b41b4-1e3c-4009-9cf5-110bcb532d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if button:\n",
    "    # get latest short feedbacks\n",
    "    latest_short_feedbacks = pick_shortfeedbacks(directory='../data', n=3)\n",
    "    # define goal from latest short feedbacks\n",
    "    learning_goals = define_goal(latest_short_feedbacks=latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64d938f0-f40c-4c11-b395-e9579fc04024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6. Use descriptive variable names.\\n7. Use consistent indentation.\\n8. Use linter to check for errors.\\n9. Use comments to explain code.\\n\\nOrganize code into separate functions, use descriptive variable names and consistent indentation, and use comments and a linter to check for errors.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c44f3f-7db4-4a90-9b8f-bb550e6e1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the program if the created output is empty\n",
    "assert len(learning_goals) != 0, 'The created learning goal was empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdd565-09be-4d42-829e-c1a50300cc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cecbbaf3-c1e0-44de-a738-676f67687019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare learning goals and latest submitted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "778c90f6-f0e4-472e-a55d-ebbf9411d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 3.05 ms, total: 14 ms\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluation = evaluate_code(directory='../data', code_input=code_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9314c948-2951-4479-9ddc-40adec9a9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe programmer has considered the learning goals when writing the provided code. The code is organized into separate functions, descriptive variable names are used, consistent indentation is used, and comments are used to explain the code. Great job!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a0fee-fae4-4465-8637-0fe17e054ae9",
   "metadata": {},
   "source": [
    "# TO INCLUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0f033-7976-47bf-81e1-0a54535ca772",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### one extensive function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ea369fa-1add-4412-ba74-a3458f7f8e85",
   "metadata": {},
   "source": [
    "# TO BE FIXED\n",
    "def define_goal(latest_short_feedbacks):\n",
    "    \n",
    "    # defining the prompt template for a standardized input\n",
    "    learning_goal_prompt = PromptTemplate(\n",
    "        input_variables=[\"short_feedback\"],\n",
    "        template=\"Please select the four most important points from this list of feedback comments. Recommendations on content are more important than recommendations on format. Give more consideration to repetitive points. {short_feedback}.\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TODO: check for optimal LLM\n",
    "    learning_goal_llm = OpenAI(temperature=0.5)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    learning_goals = learning_goal_chain.run(latest_short_feedbacks)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='learninggoals', input_string=learning_goals)\n",
    "    \n",
    "    return learning_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47cce88d-a5af-44c6-8956-399e8d9725b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.1 ms, sys: 11.6 ms, total: 64.7 ms\n",
      "Wall time: 693 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# somehow doesn't work\n",
    "learning_goals = define_goal(latest_short_feedbacks=latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a14fbf2-3e8b-4cae-beca-d3ab1d17a1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf30c9a-6a26-4eca-b0be-e9f77ec105a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### extensive step by step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10de0be5-72eb-4635-8adf-890e3d818c41",
   "metadata": {},
   "source": [
    "# works sometimes..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c84a58db-9250-4d64-b593-e5948e03bbbc",
   "metadata": {},
   "source": [
    "# defining the prompt template for a standardized input\n",
    "learning_goal_prompt = PromptTemplate(\n",
    "    input_variables=[\"short_feedback\"],\n",
    "    template=\"Please select the four most important points from this list of feedback comments. Recommendations on content are more important than recommendations on format. Give more consideration to repetitive points. {short_feedback}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "424d9666-4e34-490a-9e10-6b31a25165bc",
   "metadata": {},
   "source": [
    "# initializing the LM\n",
    "# TODO: check for optimal LLM\n",
    "learning_goal_llm = OpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b473cebc-d799-419b-a48c-a8260c750ec5",
   "metadata": {},
   "source": [
    "# a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7aa363d-ddd8-48e1-8f53-cd27a9c7ddd1",
   "metadata": {},
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "learning_goals = learning_goal_chain.run(latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "394273bc-edca-4b47-826b-7cf1903da1e2",
   "metadata": {},
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e9f71e6-99b4-4f06-9fe1-d464e4acf6bf",
   "metadata": {},
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='learninggoals', input_string=learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd0dbf-63b2-46d0-bf0c-87171805f42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e4d66e-dd26-44fc-bd02-21f77ebab332",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### short input step by step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63f1ad18-af71-46b2-9b28-0e2458ace1c1",
   "metadata": {},
   "source": [
    "# works usually..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e06b5271-7168-4a24-8d84-1aef9e7a01f1",
   "metadata": {},
   "source": [
    "# defining the prompt template for a standardized input\n",
    "learning_goal_prompt = PromptTemplate(\n",
    "    input_variables=[\"short_feedback\"],\n",
    "    template=\"Summarize the following points: {short_feedback}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ebe973-b409-4aac-8e73-1775cda8a60a",
   "metadata": {},
   "source": [
    "# initializing the LM\n",
    "# TODO: check for optimal LLM\n",
    "learning_goal_llm = OpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98eb542b-39c5-4a6e-ba23-29d8320e28a3",
   "metadata": {},
   "source": [
    "# a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87fbcd58-5065-4845-af84-1c8860903ac2",
   "metadata": {},
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "learning_goals = learning_goal_chain.run(latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3eb53307-a4d7-49ae-a5dc-0a791be7d09a",
   "metadata": {},
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45e56dd-2625-496b-94da-02a6e326b269",
   "metadata": {},
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='learninggoals', input_string=learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b06d8-c933-4d41-99bf-543d5d340038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e9ed4-1082-4ff9-8c7f-aeb383104a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
