{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f8fa04-d8cb-4beb-8e43-83728fb7a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2743c539-a08c-4cec-abcc-80391d8cc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove unnecessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "# import openai\n",
    "# import base64\n",
    "import os\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967b79b0-8b5e-47a4-911b-ad2dbfd7ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting keys\n",
    "# TODO: remove unnecessary aspects\n",
    "with open('../config.json') as f:\n",
    "    keys = json.load(f)\n",
    "PATH = keys['path']\n",
    "# openai_organization = keys['openai_organization']\n",
    "# openai.organization = openai_organization\n",
    "openai_api_key = keys['openai_api_key']\n",
    "# openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f515a5-a463-4075-a574-d55e4357bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cdcb3a-957d-4115-9a92-b5d63f73adae",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c89ceb2-6081-4861-a93c-72a3e9e37a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24478b90-b9af-429f-aabd-c179350b50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date and time as a string in the desired format\n",
    "timestamp = now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e309e23-205f-4c79-8683-b1dbc7d72692",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12a0850-0c0d-4cf3-81db-ffd7ec84b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(name, input_string):\n",
    "    # Define the file name with the timestamp\n",
    "    # TODO: add the PATH to the filename\n",
    "    filename = f\"../data/{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f54b07-ef52-4970-aaa5-3b288ab85078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedback(code_input):\n",
    "    # defining the prompt template for a standardized input\n",
    "    # TO EXPLORE: refine prompt\n",
    "    feedback_prompt = PromptTemplate(\n",
    "        input_variables=[\"code\"],\n",
    "        template=\"Please review the following code and give five recommendations with detailed explanations how to improve the programming: {code}?\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TO EXPLORE: adjust temperature\n",
    "    # TO EXPLORE: test other LMs\n",
    "    feedback_llm = OpenAI(temperature=0.3, , model_name='gpt-4')\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    feedback_chain = LLMChain(llm=feedback_llm, prompt=feedback_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    feedback = feedback_chain.run(code_input)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='feedback', input_string=feedback)\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c5c19-e166-488a-a8d1-96b925940159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15177b15-a44a-4c65-95ea-24708bee1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_feedback(feedback):\n",
    "    # defining the prompt template for a standardized input\n",
    "    short_feedback_prompt = PromptTemplate(\n",
    "        input_variables=[\"feedback\"],\n",
    "        template=\"Please shorten the aspects of the following feeback: {feedback}?\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TO OPTIMIZE\n",
    "    short_feedback_llm = OpenAI(temperature=0)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    short_feedback_chain = LLMChain(llm=short_feedback_llm, prompt=short_feedback_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    short_feedback = short_feedback_chain.run(feedback)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='shortfeedback', input_string=short_feedback)\n",
    "    \n",
    "    return short_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3b0edb-a8f7-4ff5-a60d-1bd9aac5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: separate into a reading and a merging function and reuse reading function\n",
    "def pick_shortfeedbacks(directory, n):\n",
    "    # TODO: use PATH here for directory\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_shortfeedback.txt\") and len(f) == 32]\n",
    "    \n",
    "    # Sort the list of files by date, with the most recent file first\n",
    "    files.sort(reverse=True)\n",
    "    \n",
    "    # Get the three most recent files\n",
    "    latest_files = files[:n]\n",
    "    \n",
    "    # Read the contents of the two files into string variables\n",
    "    file_contents = []\n",
    "    for file in latest_files:\n",
    "        with open(os.path.join(directory, file), \"r\") as f:\n",
    "            file_contents.append(f.read())\n",
    "            \n",
    "    # Combine the two file contents into a single string variable\n",
    "    latest_short_feedbacks = \"\\n\".join(file_contents)\n",
    "    \n",
    "    return latest_short_feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d46fb2-d747-4309-b694-a367b321429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: improve the template without making it break\n",
    "def define_goal(latest_short_feedbacks):\n",
    "    \n",
    "    # defining the prompt template for a standardized input\n",
    "    learning_goal_prompt = PromptTemplate(\n",
    "        input_variables=[\"short_feedback\"],\n",
    "        template=\"Summarize the following points: {short_feedback}\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TODO: check for optimal LLM\n",
    "    learning_goal_llm = OpenAI(temperature=0.5)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    learning_goals = learning_goal_chain.run(latest_short_feedbacks)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='learninggoals', input_string=learning_goals)\n",
    "    \n",
    "    return learning_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29074fe3-3474-4324-a2bc-77cd44ecff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what happens if there is no learning goal?\n",
    "# it is supposed to not produce an error but only return something empty\n",
    "def evaluate_code(directory, code_input):\n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    # Filter the list to only include files with the correct format\n",
    "    files = [f for f in files if f.endswith(\"_learninggoals.txt\") and len(f) == 32]\n",
    "\n",
    "    # Sort the list of files by date, with the most recent file first\n",
    "    files.sort(reverse=True)\n",
    "\n",
    "    # Get the most recent file\n",
    "    latest_file = files[:1]\n",
    "\n",
    "    with open(os.path.join(directory, latest_file[0]), \"r\") as f:\n",
    "        learning_goals = f.read()\n",
    "    \n",
    "    # defining the promp template to get both the latest code input and learning_goals\n",
    "    evaluation_prompt = PromptTemplate(\n",
    "        input_variables=[\"code_input\", \"learning_goals\"],\n",
    "        template=\"Please compare this code: {code_input} with these learning goals: {learning_goals}. If the programmer considered the learning goals when writing the provided code, say something motivating. If the programmer didn't consider the learning goals, gently remind the person of their learning goals.\",\n",
    "    )\n",
    "    \n",
    "    # application of the template\n",
    "    evaluation_prompt = evaluation_prompt.format(code_input=code_input, learning_goals=learning_goals)\n",
    "    \n",
    "    # llm definition\n",
    "    evaluation_llm = OpenAI(temperature=0.3)\n",
    "    \n",
    "    evaluation = evaluation_llm(evaluation_prompt)\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d25f0d4-052e-43ee-b685-5051ffeaa122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(code_input, directory):\n",
    "    \n",
    "    # create feedback from input\n",
    "    feedback = create_feedback(code_input=code_input)\n",
    "    \n",
    "    # stop the program if the created output is empty\n",
    "    assert len(feedback) != 0, 'The created feedback was empty'\n",
    "    \n",
    "    # create short feedback for goal definition\n",
    "    short_feedback = shorten_feedback(feedback=feedback)\n",
    "\n",
    "    # stop the program if the created output is empty\n",
    "    assert len(short_feedback) != 0, 'The created shortfeedback was empty'\n",
    "    \n",
    "    # evaluate if the learning goals have been applied while coding\n",
    "    evaluation = evaluate_code(directory=directory, code_input=code_input)\n",
    "    \n",
    "    return feedback, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2abdf3-54d5-4ee8-ac56-fec41e861e3d",
   "metadata": {},
   "source": [
    "# Code review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7362f9-f9f2-43f3-b704-0e5368ca8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature to be added: read from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "674d4678-abee-481a-9b18-d7bfe423330a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add code input as text here\n",
    "code_input = str('''\n",
    "def save_to_txt(name, input_string, timestamp):\n",
    "    # Define the file name with the timestamp\n",
    "    # TODO: add the PATH to the filename\n",
    "    filename = f\"../data/{timestamp}_{name}.txt\"\n",
    "    # Write the string to the file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(input_string) \n",
    "        '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f0ca9b8-42d6-4b83-bbb4-6220a4a8dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='codeinput', input_string=code_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9d3b92-ed62-4e46-b25c-22319fc5984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Move the functions outside of the main code block. This will make the code easier to read and debug. \n",
      "2. Add comments to explain what each function does. This will help other developers understand the code. \n",
      "3. Use descriptive variable names. This will make the code easier to read and understand. \n",
      "4. Use more descriptive names for the functions. This will make the code easier to read and understand. \n",
      "5. Add error handling for invalid inputs. This will help ensure that the code runs correctly and produces the expected results.\n",
      "\n",
      "\n",
      "The programmer appears to have considered the learning goals when writing the provided code. The code is organized into separate functions, uses descriptive variable names, and consistent indentation. Additionally, the code is commented to explain the code and a linter is used to check for errors. Well done!\n",
      "CPU times: user 71.2 ms, sys: 17.9 ms, total: 89.1 ms\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# conditional execution of code_input was passed\n",
    "if code_input is not None:\n",
    "    \n",
    "    feedback, evaluation = get_feedback(code_input=code_input, directory='../data')\n",
    "    \n",
    "    print(feedback)\n",
    "    print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536255f-8a9e-4a33-88c3-9d0180ee0c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set learning goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbd79ef-9256-4ba2-a5bc-d298ea5e8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be done once and then only if the user checks a button like \"generate learning targets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6fe1f39-3180-410e-b551-88edadd7df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently pseudo-button:\n",
    "button = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7b41b4-1e3c-4009-9cf5-110bcb532d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your new goals are:\n",
      "\n",
      "Organize code into separate functions to improve readability and maintainability, use descriptive variable and function names, and include comments to explain code and error handling for invalid inputs.\n",
      "CPU times: user 12.4 ms, sys: 4.51 ms, total: 16.9 ms\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if button:\n",
    "    # get latest short feedbacks\n",
    "    latest_short_feedbacks = pick_shortfeedbacks(directory='../data', n=3)\n",
    "    # define goal from latest short feedbacks\n",
    "    learning_goals = define_goal(latest_short_feedbacks=latest_short_feedbacks)\n",
    "    \n",
    "    # stop the program if the created output is empty\n",
    "    assert len(learning_goals) != 0, 'The created learning goal was empty'\n",
    "    \n",
    "    print('Your new goals are:' + learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdd565-09be-4d42-829e-c1a50300cc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c0a0fee-fae4-4465-8637-0fe17e054ae9",
   "metadata": {},
   "source": [
    "# TO INCLUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0f033-7976-47bf-81e1-0a54535ca772",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### one extensive function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ea369fa-1add-4412-ba74-a3458f7f8e85",
   "metadata": {},
   "source": [
    "# TO BE FIXED\n",
    "def define_goal(latest_short_feedbacks):\n",
    "    \n",
    "    # defining the prompt template for a standardized input\n",
    "    learning_goal_prompt = PromptTemplate(\n",
    "        input_variables=[\"short_feedback\"],\n",
    "        template=\"Please select the four most important points from this list of feedback comments. Recommendations on content are more important than recommendations on format. Give more consideration to repetitive points. {short_feedback}.\",\n",
    "    )\n",
    "    \n",
    "    # initializing the LM\n",
    "    # TODO: check for optimal LLM\n",
    "    learning_goal_llm = OpenAI(temperature=0.5)\n",
    "    \n",
    "    # a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "    learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)\n",
    "    \n",
    "    # Run the chain only specifying the input variable.\n",
    "    learning_goals = learning_goal_chain.run(latest_short_feedbacks)\n",
    "    \n",
    "    # saving the input to txt via the prepared function\n",
    "    save_to_txt(name='learninggoals', input_string=learning_goals)\n",
    "    \n",
    "    return learning_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47cce88d-a5af-44c6-8956-399e8d9725b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.1 ms, sys: 11.6 ms, total: 64.7 ms\n",
      "Wall time: 693 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# somehow doesn't work\n",
    "learning_goals = define_goal(latest_short_feedbacks=latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a14fbf2-3e8b-4cae-beca-d3ab1d17a1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf30c9a-6a26-4eca-b0be-e9f77ec105a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### extensive step by step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10de0be5-72eb-4635-8adf-890e3d818c41",
   "metadata": {},
   "source": [
    "# works sometimes..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c84a58db-9250-4d64-b593-e5948e03bbbc",
   "metadata": {},
   "source": [
    "# defining the prompt template for a standardized input\n",
    "learning_goal_prompt = PromptTemplate(\n",
    "    input_variables=[\"short_feedback\"],\n",
    "    template=\"Please select the four most important points from this list of feedback comments. Recommendations on content are more important than recommendations on format. Give more consideration to repetitive points. {short_feedback}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "424d9666-4e34-490a-9e10-6b31a25165bc",
   "metadata": {},
   "source": [
    "# initializing the LM\n",
    "# TODO: check for optimal LLM\n",
    "learning_goal_llm = OpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b473cebc-d799-419b-a48c-a8260c750ec5",
   "metadata": {},
   "source": [
    "# a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7aa363d-ddd8-48e1-8f53-cd27a9c7ddd1",
   "metadata": {},
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "learning_goals = learning_goal_chain.run(latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "394273bc-edca-4b47-826b-7cf1903da1e2",
   "metadata": {},
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e9f71e6-99b4-4f06-9fe1-d464e4acf6bf",
   "metadata": {},
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='learninggoals', input_string=learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd0dbf-63b2-46d0-bf0c-87171805f42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e4d66e-dd26-44fc-bd02-21f77ebab332",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### short input step by step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63f1ad18-af71-46b2-9b28-0e2458ace1c1",
   "metadata": {},
   "source": [
    "# works usually..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e06b5271-7168-4a24-8d84-1aef9e7a01f1",
   "metadata": {},
   "source": [
    "# defining the prompt template for a standardized input\n",
    "learning_goal_prompt = PromptTemplate(\n",
    "    input_variables=[\"short_feedback\"],\n",
    "    template=\"Summarize the following points: {short_feedback}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ebe973-b409-4aac-8e73-1775cda8a60a",
   "metadata": {},
   "source": [
    "# initializing the LM\n",
    "# TODO: check for optimal LLM\n",
    "learning_goal_llm = OpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98eb542b-39c5-4a6e-ba23-29d8320e28a3",
   "metadata": {},
   "source": [
    "# a simple chain taking user input, formatting the prompt and sending it to the LM\n",
    "learning_goal_chain = LLMChain(llm=learning_goal_llm, prompt=learning_goal_prompt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87fbcd58-5065-4845-af84-1c8860903ac2",
   "metadata": {},
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "learning_goals = learning_goal_chain.run(latest_short_feedbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3eb53307-a4d7-49ae-a5dc-0a791be7d09a",
   "metadata": {},
   "source": [
    "learning_goals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45e56dd-2625-496b-94da-02a6e326b269",
   "metadata": {},
   "source": [
    "# saving the input to txt via the prepared function\n",
    "save_to_txt(name='learninggoals', input_string=learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b06d8-c933-4d41-99bf-543d5d340038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e9ed4-1082-4ff9-8c7f-aeb383104a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
